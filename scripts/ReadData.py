import pandas as pd
from sklearn.model_selection import train_test_split
import os
import cv2
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor
import functools


class ReadData:
    # --- Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN ---
    GT_PATH = 'dataset/ISIC_2024_Training_GroundTruth.csv'
    IMAGES_DIR = 'dataset/ISIC_2024_Training_Input'

    # ThÆ° má»¥c lÆ°u áº£nh sau khi Ä‘Ã£ Clean (XÃ³a lÃ´ng + Resize)
    OUTPUT_IMG_DIR = 'dataset/ISIC_Processed_Images'

    # ThÆ° má»¥c lÆ°u file CSV (metadata)
    CSV_OUTPUT_DIR = 'dataset_splits'

    ID_COLUMN = 'isic_id'
    TARGET_COLUMN = 'malignant'

    @classmethod
    def load_metadata(cls):
        try:
            df = pd.read_csv(cls.GT_PATH)
            # Táº¡o Ä‘Æ°á»ng dáº«n Ä‘áº§y Ä‘á»§ tá»›i áº£nh gá»‘c
            df['image_path'] = df[cls.ID_COLUMN].apply(lambda x: os.path.join(cls.IMAGES_DIR, f"{x}.jpg"))
            print(f"âœ… ÄÃ£ táº£i metadata: {len(df)} áº£nh.")
            return df
        except Exception as e:
            print(f"âŒ Lá»—i táº£i CSV gá»‘c: {e}")
            return None

    @classmethod
    def split_data(cls, df):
        """Chia Stratified: Train/Val/Test"""
        # Giá»¯ nguyÃªn logic chia táº­p dá»¯ liá»‡u cá»§a báº¡n
        train_val, test = train_test_split(df, test_size=0.2, stratify=df[cls.TARGET_COLUMN], random_state=42)
        train, val = train_test_split(train_val, test_size=0.125, stratify=train_val[cls.TARGET_COLUMN],
                                      random_state=42)

        print(f"ğŸ“Š Thá»‘ng kÃª: Train={len(train)}, Val={len(val)}, Test={len(test)}")
        return train, val, test

    # --- WORKER Xá»¬ LÃ áº¢NH (CLEAN ONLY) ---
    @staticmethod
    def remove_hair(image):
        """Thuáº­t toÃ¡n xÃ³a lÃ´ng (Giá»¯ nguyÃªn)"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
            blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, kernel)
            blackhat = cv2.GaussianBlur(blackhat, (3, 3), 0)
            _, thresh = cv2.threshold(blackhat, 10, 255, cv2.THRESH_BINARY)
            return cv2.inpaint(image, thresh, 1, cv2.INPAINT_TELEA)
        except:
            return image

    @staticmethod
    def _process_worker(row_tuple, output_dir):
        """
        Äá»c áº£nh gá»‘c -> Resize 300x300 -> XÃ³a lÃ´ng -> LÆ°u ra file má»›i.
        Má»¥c Ä‘Ã­ch: Giáº£m táº£i cho CPU khi train (khÃ´ng pháº£i resize/xÃ³a lÃ´ng on-the-fly).
        """
        idx, row = row_tuple
        src_path = row['image_path']
        fname = os.path.basename(src_path)
        dst_path = os.path.join(output_dir, fname)

        # Resume: Náº¿u áº£nh Ä‘Ã£ xá»­ lÃ½ rá»“i thÃ¬ bá» qua
        if os.path.exists(dst_path): return dst_path

        try:
            img = cv2.imread(src_path)
            if img is not None:
                # Resize vá» 300x300 Ä‘á»ƒ nháº¹ á»• cá»©ng vÃ  load nhanh hÆ¡n
                img = cv2.resize(img, (300, 300))

                # XÃ³a lÃ´ng (Pre-processing tÄ©nh)
                clean = ReadData.remove_hair(img)

                cv2.imwrite(dst_path, clean)
                return dst_path
        except:
            pass
        return src_path  # Fallback náº¿u lá»—i

    @classmethod
    def clean_dataset(cls, df, folder_name):
        """Cháº¡y Ä‘a luá»“ng Ä‘á»ƒ clean áº£nh"""
        save_dir = os.path.join(cls.OUTPUT_IMG_DIR, folder_name)
        os.makedirs(save_dir, exist_ok=True)

        print(f"ğŸ§¹ Äang xá»­ lÃ½ (Clean & Resize) {len(df)} áº£nh vÃ o '{folder_name}'...")

        with ProcessPoolExecutor(max_workers=os.cpu_count()) as ex:
            func = functools.partial(cls._process_worker, output_dir=save_dir)
            new_paths = list(tqdm(ex.map(func, df.iterrows()), total=len(df)))

        # Cáº­p nháº­t Ä‘Æ°á»ng dáº«n trong DataFrame sang áº£nh Ä‘Ã£ clean
        df_new = df.copy()
        df_new['image_path'] = new_paths
        return df_new

    @classmethod
    def run(cls):
        print("ğŸš€ Báº¯t Ä‘áº§u quy trÃ¬nh chuáº©n bá»‹ dá»¯ liá»‡u (Online Augmentation Ready)...")

        # 1. Load
        df = cls.load_metadata()
        if df is None: return False

        # 2. Split
        train, val, test = cls.split_data(df)

        # 3. Clean (Chá»‰ Pre-process tÄ©nh, KHÃ”NG Augment sinh áº£nh má»›i)
        train = cls.clean_dataset(train, 'Train_Clean')
        val = cls.clean_dataset(val, 'Val_Clean')
        test = cls.clean_dataset(test, 'Test_Clean')

        # 4. Save CSV (LÆ°u danh sÃ¡ch file gá»‘c + Ä‘Æ°á»ng dáº«n áº£nh clean)
        os.makedirs(cls.CSV_OUTPUT_DIR, exist_ok=True)
        print(f"ğŸ’¾ Äang lÆ°u CSV vÃ o {cls.CSV_OUTPUT_DIR}...")

        train.to_csv(f'{cls.CSV_OUTPUT_DIR}/processed_train.csv', index=False)
        val.to_csv(f'{cls.CSV_OUTPUT_DIR}/processed_val.csv', index=False)
        test.to_csv(f'{cls.CSV_OUTPUT_DIR}/processed_test.csv', index=False)

        print("âœ… HoÃ n táº¥t! Dá»¯ liá»‡u Ä‘Ã£ sáºµn sÃ ng cho Dataset Class.")
        return True


if __name__ == '__main__':
    ReadData.run()