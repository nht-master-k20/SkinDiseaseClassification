# Metric: ÄÃ¡nh giÃ¡ báº±ng pAUC (max_fpr=0.01) thay vÃ¬ F1.
# Data: Sá»­ dá»¥ng Online Augmentation (is_train=True cho táº­p train).
# Sampler: KHÃ”NG dÃ¹ng WeightedRandomSampler (Ä‘á»ƒ Shuffle=True máº·c Ä‘á»‹nh) -> ÄÃ¢y lÃ  Baseline thuáº§n.
# Loss: CrossEntropyLoss (CÃ³ thá»ƒ dÃ¹ng class weights nháº¹ Ä‘á»ƒ model há»™i tá»¥, nhÆ°ng khÃ´ng dÃ¹ng Focal Loss).
# Reproducibility: ThÃªm hÃ m seed_everything Ä‘á»ƒ Ä‘áº£m báº£o chuá»—i Augmentation giá»‘ng nhau cho cÃ¡c Version sau.

import os
import random
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import timm
import mlflow
import mlflow.pytorch
from torch.utils.data import DataLoader
from torch.amp import GradScaler, autocast
from sklearn.metrics import f1_score, accuracy_score, classification_report, roc_auc_score
from sklearn.utils.class_weight import compute_class_weight
from scripts.ISICDataset import ISICDataset


# --- 0. SEED CONTROL ---
def seed_everything(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


# --- 1. LOGGING (CLEANED) ---
def log_training_params(version, batch_size, epochs, lr, class_weights=None):
    """Chá»‰ log nhá»¯ng tham sá»‘ quan trá»ng thay Ä‘á»•i giá»¯a cÃ¡c version"""
    params = {
        "version": version,
        "model": "EfficientNet-B3",
        "loss_function": "CrossEntropyLoss",
        "sampler": "OFF (Shuffle=True)",
        "augmentation": "Online (Albumentations)",
        "batch_size": batch_size,
        "epochs": epochs,
        "learning_rate": lr,
    }
    if class_weights is not None:
        params["class_weights"] = f"Benign:{class_weights[0]:.2f} | Malignant:{class_weights[1]:.2f}"
    mlflow.log_params(params)


def calculate_metrics(y_true, y_probs, threshold=0.5):
    y_pred = (y_probs >= threshold).astype(int)
    try:
        pauc = roc_auc_score(y_true, y_probs, max_fpr=0.01)
        auc = roc_auc_score(y_true, y_probs)
    except:
        pauc, auc = 0.0, 0.0

    return {
        "pauc_0.01": pauc,
        "auc": auc,
        "f1_malignant": f1_score(y_true, y_pred, labels=[1], average='binary', zero_division=0),
        "accuracy": accuracy_score(y_true, y_pred)
    }


# --- 2. TRAINING CORE ---
def train_one_epoch(model, loader, optimizer, criterion, scaler):
    model.train()
    total_loss, count = 0.0, 0

    for imgs, labels in loader:
        imgs, labels = imgs.cuda(non_blocking=True), labels.cuda(non_blocking=True)

        optimizer.zero_grad()
        with autocast(device_type='cuda'):
            outputs = model(imgs)
            loss = criterion(outputs, labels)

        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Hardcode clip=1.0 cho gá»n
        scaler.step(optimizer)
        scaler.update()

        total_loss += loss.item()
        count += 1

    return total_loss / max(1, count)


def validate(model, loader, criterion):
    model.eval()
    total_loss = 0.0
    all_probs, all_labels = [], []

    with torch.no_grad():
        for imgs, labels in loader:
            imgs, labels = imgs.cuda(non_blocking=True), labels.cuda(non_blocking=True)
            outputs = model(imgs)
            loss = criterion(outputs, labels)
            total_loss += loss.item()

            # Láº¥y xÃ¡c suáº¥t lá»›p 1 (Ãc tÃ­nh)
            probs = torch.softmax(outputs, dim=1)[:, 1]
            all_probs.extend(probs.cpu().tolist())
            all_labels.extend(labels.cpu().tolist())

    return total_loss / max(1, len(loader)), np.array(all_labels), np.array(all_probs)


# --- 3. MAIN (SIMPLIFIED) ---
def train(image_size=300, batch_size=32, epochs=10, base_lr=1e-3):
    seed_everything(42)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ðŸ–¥ï¸ Running V1 Baseline on {device}...")

    # MLflow Setup
    os.environ["DATABRICKS_HOST"] = "https://dbc-cba55001-5dea.cloud.databricks.com"
    os.environ["DATABRICKS_TOKEN"] = "dapif865faf65e4f29f9f213de9b6f2ffa3c"
    mlflow.set_tracking_uri("databricks")
    mlflow.set_experiment("/Workspace/Users/nht.master.k20@gmail.com/v1")

    # Paths (Máº·c Ä‘á»‹nh dÃ¹ng dataset_splits)
    CSV_DIR = 'dataset_splits'
    train_df = pd.read_csv(f'{CSV_DIR}/processed_train.csv')
    val_df = pd.read_csv(f'{CSV_DIR}/processed_val.csv')
    test_df = pd.read_csv(f'{CSV_DIR}/processed_test.csv')
    print(f"ðŸ“Š Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}")

    # Loaders
    train_loader = DataLoader(ISICDataset(train_df, image_size, is_train=True),
                              batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)
    val_loader = DataLoader(ISICDataset(val_df, image_size, is_train=False),
                            batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)
    test_loader = DataLoader(ISICDataset(test_df, image_size, is_train=False),
                             batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)

    # Model Setup
    model = timm.create_model("tf_efficientnet_b3.ns_jft_in1k", pretrained=True, num_classes=2).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, weight_decay=0.01)

    # Class Weights
    y_train = train_df['malignant'].values
    cw = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
    criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(cw).to(device))

    scaler = GradScaler()
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)

    # Start Run
    with mlflow.start_run(run_name="V1_Baseline"):
        log_training_params("V1_Baseline", batch_size, epochs, base_lr, cw)

        best_pauc = -1
        model_path = "checkpoints/best_v1.pth"
        os.makedirs("checkpoints", exist_ok=True)

        for epoch in range(epochs):
            lr = optimizer.param_groups[0]['lr']
            scheduler.step()

            # Train & Val
            train_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler)
            val_loss, val_labels, val_probs = validate(model, val_loader, criterion)

            # Metrics
            metrics = calculate_metrics(val_labels, val_probs)
            current_pauc = metrics['pauc_0.01']

            # Log Metrics (ThÃªm prefix 'val_' tá»± Ä‘á»™ng khi log)
            mlflow.log_metrics({f"val_{k}": v for k, v in metrics.items()}, step=epoch)
            mlflow.log_metrics({"train_loss": train_loss, "val_loss": val_loss}, step=epoch)

            print(f"Epoch [{epoch + 1}/{epochs}] | pAUC: {current_pauc:.4f} | AUC: {metrics['auc']:.4f}")

            if current_pauc > best_pauc:
                best_pauc = current_pauc
                torch.save(model.state_dict(), model_path)
                print(f"  ðŸ”¥ Saved Best Model (pAUC: {best_pauc:.4f})")

        # Test
        print("\nðŸ§ª Testing Best Model...")
        model.load_state_dict(torch.load(model_path))
        test_loss, test_labels, test_probs = validate(model, test_loader, criterion)
        test_metrics = calculate_metrics(test_labels, test_probs)

        print(f"ðŸ† FINAL TEST pAUC: {test_metrics['pauc_0.01']:.4f}")
        print(classification_report(test_labels, (test_probs >= 0.5).astype(int), target_names=['Benign', 'Malignant']))

        mlflow.log_metrics({f"test_{k}": v for k, v in test_metrics.items()})


if __name__ == '__main__':
    train()