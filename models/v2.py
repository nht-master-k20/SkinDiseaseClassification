# Chiáº¿n thuáº­t: Sá»­ dá»¥ng Cross Entropy Loss káº¿t há»£p Weighted Random Sampler (Äá»ƒ batch luÃ´n cÃ¢n báº±ng 50/50).
# Dá»¯ liá»‡u: Báº­t Online Augmentation (is_train=True).
# Metric: Tá»‘i Æ°u hÃ³a theo pAUC (0.01).
# LÆ°u Ã½ quan trá»ng: Khi Ä‘Ã£ dÃ¹ng WeightedRandomSampler, ta khÃ´ng cáº§n truyá»n weight vÃ o hÃ m Loss ná»¯a (vÃ¬ dá»¯ liá»‡u vÃ o model Ä‘Ã£ Ä‘Æ°á»£c cÃ¢n báº±ng rá»“i).

import os
import random
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import timm
import mlflow
import mlflow.pytorch
from torch.utils.data import DataLoader, WeightedRandomSampler
from torch.amp import GradScaler, autocast
from sklearn.metrics import f1_score, accuracy_score, classification_report, roc_auc_score, recall_score
from scripts.ISICDataset import ISICDataset


# --- 0. SEED CONTROL ---
def seed_everything(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


# --- 1. LOGGING ---
def log_training_params(version, batch_size, epochs, lr):
    """Log cÃ¡c tham sá»‘ Ä‘áº·c trÆ°ng cá»§a V2"""
    params = {
        "version": version,
        "model": "EfficientNet-B3",
        "loss_function": "CrossEntropyLoss",  # V2 dÃ¹ng CE bÃ¬nh thÆ°á»ng
        "sampler": "WeightedRandomSampler (Balance 50/50)",  # Äiá»ƒm nháº¥n cá»§a V2
        "augmentation": "Online (Albumentations)",
        "batch_size": batch_size,
        "epochs": epochs,
        "learning_rate": lr,
        "metric_target": "pAUC (0.01)"
    }
    mlflow.log_params(params)


def calculate_metrics(y_true, y_probs, threshold=0.5):
    y_pred = (y_probs >= threshold).astype(int)
    try:
        pauc = roc_auc_score(y_true, y_probs, max_fpr=0.01)
        auc = roc_auc_score(y_true, y_probs)
    except:
        pauc, auc = 0.0, 0.0

    return {
        "pauc_0.01": pauc,
        "auc": auc,
        "f1_malignant": f1_score(y_true, y_pred, labels=[1], average='binary', zero_division=0),
        "recall_malignant": recall_score(y_true, y_pred, labels=[1], average='binary', zero_division=0),
        "accuracy": accuracy_score(y_true, y_pred)
    }


# --- 2. TRAINING CORE ---
def train_one_epoch(model, loader, optimizer, criterion, scaler):
    model.train()
    total_loss, count = 0.0, 0

    for imgs, labels in loader:
        imgs, labels = imgs.cuda(non_blocking=True), labels.cuda(non_blocking=True)

        optimizer.zero_grad()
        with autocast(device_type='cuda'):
            outputs = model(imgs)
            loss = criterion(outputs, labels)

        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        scaler.step(optimizer)
        scaler.update()

        total_loss += loss.item()
        count += 1

    return total_loss / max(1, count)


def validate(model, loader, criterion):
    model.eval()
    total_loss = 0.0
    all_probs, all_labels = [], []

    with torch.no_grad():
        for imgs, labels in loader:
            imgs, labels = imgs.cuda(non_blocking=True), labels.cuda(non_blocking=True)
            outputs = model(imgs)
            loss = criterion(outputs, labels)
            total_loss += loss.item()

            # Láº¥y xÃ¡c suáº¥t lá»›p 1
            probs = torch.softmax(outputs, dim=1)[:, 1]
            all_probs.extend(probs.cpu().tolist())
            all_labels.extend(labels.cpu().tolist())

    return total_loss / max(1, len(loader)), np.array(all_labels), np.array(all_probs)


# --- 3. MAIN V2 ---
def train(image_size=300, batch_size=32, epochs=10, base_lr=1e-3):
    seed_everything(42)  # Cá»‘ Ä‘á»‹nh háº¡t giá»‘ng
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ðŸ–¥ï¸ Running V2 (Sampler + CE) on {device}...")

    # MLflow Setup
    os.environ["DATABRICKS_HOST"] = "https://dbc-cba55001-5dea.cloud.databricks.com"
    os.environ["DATABRICKS_TOKEN"] = "dapif865faf65e4f29f9f213de9b6f2ffa3c"
    mlflow.set_tracking_uri("databricks")
    mlflow.set_experiment("/Workspace/Users/nht.master.k20@gmail.com/v2")

    # Paths
    CSV_DIR = 'dataset_splits'
    train_df = pd.read_csv(f'{CSV_DIR}/processed_train.csv')
    val_df = pd.read_csv(f'{CSV_DIR}/processed_val.csv')
    test_df = pd.read_csv(f'{CSV_DIR}/processed_test.csv')
    print(f"ðŸ“Š Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}")

    # --- Cáº¤U HÃŒNH SAMPLER (ÄIá»‚M KHÃC BIá»†T CHÃNH Cá»¦A V2) ---
    print("âš–ï¸ Configuring WeightedRandomSampler...")
    y_train = train_df['malignant'].values.astype(int)
    class_counts = np.bincount(y_train)
    # Trá»ng sá»‘ nghá»‹ch Ä‘áº£o vá»›i sá»‘ lÆ°á»£ng máº«u (Lá»›p Ã­t -> Trá»ng sá»‘ cao)
    class_weights = 1. / class_counts
    sample_weights = class_weights[y_train]

    sampler = WeightedRandomSampler(
        weights=torch.DoubleTensor(sample_weights),
        num_samples=len(sample_weights),
        replacement=True
    )
    # --------------------------------------------------------

    # Loaders
    # LÆ¯U Ã: Khi dÃ¹ng Sampler thÃ¬ shuffle pháº£i lÃ  False
    train_loader = DataLoader(
        ISICDataset(train_df, image_size, is_train=True),  # Online Augmentation ON
        batch_size=batch_size,
        sampler=sampler,  # Sampler ON
        shuffle=False,  # Báº¯t buá»™c False khi cÃ³ Sampler
        num_workers=8, pin_memory=True
    )

    val_loader = DataLoader(ISICDataset(val_df, image_size, is_train=False),
                            batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)
    test_loader = DataLoader(ISICDataset(test_df, image_size, is_train=False),
                             batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)

    # Model
    model = timm.create_model("tf_efficientnet_b3.ns_jft_in1k", pretrained=True, num_classes=2).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, weight_decay=0.01)

    # Loss V2: CrossEntropyLoss KHÃ”NG weight (VÃ¬ Sampler Ä‘Ã£ cÃ¢n báº±ng batch rá»“i)
    criterion = nn.CrossEntropyLoss()

    scaler = GradScaler()
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)

    # Run MLflow
    with mlflow.start_run(run_name="V2_Sampler_CE"):
        log_training_params("V2_Sampler_CE", batch_size, epochs, base_lr)

        best_pauc = -1
        model_path = "checkpoints/best_v2.pth"
        os.makedirs("checkpoints", exist_ok=True)

        for epoch in range(epochs):
            lr = optimizer.param_groups[0]['lr']
            scheduler.step()

            # Train & Val
            train_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler)
            val_loss, val_labels, val_probs = validate(model, val_loader, criterion)

            # Metrics
            metrics = calculate_metrics(val_labels, val_probs)
            current_pauc = metrics['pauc_0.01']

            # Log
            mlflow.log_metrics({f"val_{k}": v for k, v in metrics.items()}, step=epoch)
            mlflow.log_metrics({"train_loss": train_loss, "val_loss": val_loss}, step=epoch)

            print(f"Epoch [{epoch + 1}/{epochs}] | pAUC: {current_pauc:.4f} | AUC: {metrics['auc']:.4f}")

            # Save Best by pAUC
            if current_pauc > best_pauc:
                best_pauc = current_pauc
                torch.save(model.state_dict(), model_path)
                print(f"  ðŸ”¥ Saved Best Model (pAUC: {best_pauc:.4f})")

        # Final Test
        print("\nðŸ§ª Testing Best Model V2...")
        model.load_state_dict(torch.load(model_path))
        test_loss, test_labels, test_probs = validate(model, test_loader, criterion)
        test_metrics = calculate_metrics(test_labels, test_probs)

        print(f"ðŸ† FINAL TEST V2 pAUC: {test_metrics['pauc_0.01']:.4f}")
        print(classification_report(test_labels, (test_probs >= 0.5).astype(int), target_names=['Benign', 'Malignant']))

        mlflow.log_metrics({f"test_{k}": v for k, v in test_metrics.items()})


if __name__ == '__main__':
    train()