# Ki·∫øn tr√∫c: Chuy·ªÉn sang num_classes=1 (Binary Mode). ƒê√¢y l√† c·∫•u h√¨nh chuy√™n nghi·ªáp cho b√†i to√°n 2 l·ªõp, gi√∫p vi·ªác t√≠nh to√°n Loss v√† Bias Init ch√≠nh x√°c h∆°n so v·ªõi ƒë·ªÉ 2 output nodes.
# Loss Function: S·ª≠ d·ª•ng Binary Focal Loss.
# V√¨ ƒë√£ d√πng Sampler (c√¢n b·∫±ng s·ªë l∆∞·ª£ng 50/50), Focal Loss ·ªü ƒë√¢y ƒë√≥ng vai tr√≤ t·∫≠p trung v√†o "Hard Examples" (nh·ªØng ca kh√≥ ph√¢n bi·ªát) thay v√¨ c√¢n b·∫±ng d·ªØ li·ªáu.
# Bias Initialization: Kh·ªüi t·∫°o bias l·ªõp cu·ªëi c√πng ƒë·ªÉ output ban ƒë·∫ßu c·ªßa model c√≥ x√°c su·∫•t ~1% (prior probability). ƒêi·ªÅu n√†y gi√∫p Loss kh√¥ng b·ªã "n·ªï" (explosion) ·ªü nh·ªØng epoch ƒë·∫ßu, gi√∫p model h·ªôi t·ª• m∆∞·ª£t h∆°n.

import sys
import os
import random
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import timm
import mlflow
import mlflow.pytorch
from torch.utils.data import DataLoader, WeightedRandomSampler
from torch.cuda.amp import GradScaler, autocast
from sklearn.metrics import f1_score, accuracy_score, classification_report, roc_auc_score, recall_score
from scripts.ISICDataset import ISICDataset


# --- 0. SEED CONTROL ---
def seed_everything(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


# --- 1. COMPONENTS ---
class BinaryFocalLoss(nn.Module):
    def __init__(self, alpha=0.5, gamma=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        targets = targets.to(inputs.device)
        bce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-bce_loss)
        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)
        focal_loss = alpha_t * (1 - pt) ** self.gamma * bce_loss

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


def initialize_bias(model, device):
    prior = 0.01
    bias_value = -np.log((1 - prior) / prior)
    if hasattr(model, 'classifier') and isinstance(model.classifier, nn.Linear):
        with torch.no_grad():
            model.classifier.bias.data.fill_(bias_value)
            print(f"üîß Bias Initialized to {bias_value:.4f}")
    elif hasattr(model, 'fc') and isinstance(model.fc, nn.Linear):
        with torch.no_grad():
            model.fc.bias.data.fill_(bias_value)
            print(f"üîß Bias Initialized to {bias_value:.4f}")
    model.to(device)
    return model


def calculate_metrics(y_true, y_probs, threshold=0.5):
    # T√≠nh to√°n metrics d·ª±a tr√™n ng∆∞·ª°ng (threshold) ƒë∆∞·ª£c truy·ªÅn v√†o
    y_pred = (y_probs >= threshold).astype(int)
    try:
        pauc = roc_auc_score(y_true, y_probs, max_fpr=0.01)
        auc = roc_auc_score(y_true, y_probs)
    except:
        pauc, auc = 0.0, 0.0

    return {
        "pauc_0.01": pauc,
        "auc": auc,
        "f1_malignant": f1_score(y_true, y_pred, labels=[1], average='binary', zero_division=0),
        "recall_malignant": recall_score(y_true, y_pred, labels=[1], average='binary', zero_division=0),
        "accuracy": accuracy_score(y_true, y_pred)
    }


def log_training_params(version, batch_size, epochs, lr):
    params = {
        "version": version,
        "loss": "BinaryFocalLoss",
        "sampler": "WeightedRandomSampler",
        "metric": "pAUC (0.01)"
    }
    mlflow.log_params(params)


# --- 2. TRAIN STEP ---
def train_one_epoch(model, loader, optimizer, criterion, scaler):
    model.train()
    total_loss, count = 0.0, 0

    for imgs, labels in loader:
        imgs = imgs.cuda(non_blocking=True)
        labels = labels.cuda(non_blocking=True)
        labels = labels.float().unsqueeze(1)

        optimizer.zero_grad()
        with autocast():
            outputs = model(imgs)
            loss = criterion(outputs, labels)

        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        scaler.step(optimizer)
        scaler.update()

        total_loss += loss.item()
        count += 1

    return total_loss / max(1, count)


def validate(model, loader, criterion):
    model.eval()
    total_loss = 0.0
    all_probs, all_labels = [], []

    with torch.no_grad():
        for imgs, labels in loader:
            imgs = imgs.cuda(non_blocking=True)
            labels = labels.cuda(non_blocking=True)
            labels_float = labels.float().unsqueeze(1)

            outputs = model(imgs)
            loss = criterion(outputs, labels_float)
            total_loss += loss.item()

            probs = torch.sigmoid(outputs).cpu().numpy().flatten()
            all_probs.extend(probs)
            all_labels.extend(labels.cpu().numpy())

    return total_loss / max(1, len(loader)), np.array(all_labels), np.array(all_probs)


# --- 3. MAIN TRAIN ---
def train(image_size=300, batch_size=32, epochs=10, base_lr=1e-3):
    seed_everything(42)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üñ•Ô∏è Running V3 (Advanced) on {device}...")

    # MLflow Setup
    os.environ["DATABRICKS_HOST"] = "https://dbc-cba55001-5dea.cloud.databricks.com"
    os.environ["DATABRICKS_TOKEN"] = "dapif865faf65e4f29f9f213de9b6f2ffa3c"
    mlflow.set_tracking_uri("databricks")
    mlflow.set_experiment("/Workspace/Users/nht.master.k20@gmail.com/v3")

    CSV_DIR = 'dataset_splits'
    train_df = pd.read_csv(f'{CSV_DIR}/processed_train.csv')
    val_df = pd.read_csv(f'{CSV_DIR}/processed_val.csv')
    test_df = pd.read_csv(f'{CSV_DIR}/processed_test.csv')
    print(f"üìä Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}")

    # Sampler
    y_train = train_df['malignant'].values.astype(int)
    class_counts = np.bincount(y_train)
    sample_weights = 1. / class_counts[y_train]
    sampler = WeightedRandomSampler(torch.DoubleTensor(sample_weights), len(sample_weights), replacement=True)

    # Loaders
    train_loader = DataLoader(ISICDataset(train_df, image_size, is_train=True),
                              batch_size=batch_size, sampler=sampler, shuffle=False,
                              num_workers=8, pin_memory=True)
    val_loader = DataLoader(ISICDataset(val_df, image_size, is_train=False),
                            batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)
    test_loader = DataLoader(ISICDataset(test_df, image_size, is_train=False),
                             batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)

    # Model
    model = timm.create_model("tf_efficientnet_b3.ns_jft_in1k", pretrained=True, num_classes=1)
    model = initialize_bias(model, device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, weight_decay=0.01)
    criterion = BinaryFocalLoss(alpha=0.5, gamma=2.0)
    scaler = GradScaler()
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)

    with mlflow.start_run(run_name="V3_Advanced"):
        log_training_params("V3_Advanced", batch_size, epochs, base_lr)

        best_pauc = -1
        model_path = "checkpoints/best_v3.pth"
        os.makedirs("checkpoints", exist_ok=True)

        for epoch in range(epochs):
            lr = optimizer.param_groups[0]['lr']

            # Train
            train_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler)
            scheduler.step()

            # Validate
            val_loss, val_labels, val_probs = validate(model, val_loader, criterion)

            # Metrics (M·∫∑c ƒë·ªãnh log theo threshold 0.5 trong qu√° tr√¨nh train ƒë·ªÉ tham kh·∫£o)
            metrics = calculate_metrics(val_labels, val_probs)
            current_pauc = metrics['pauc_0.01']

            mlflow.log_metrics({f"val_{k}": v for k, v in metrics.items()}, step=epoch)
            mlflow.log_metrics({"train_loss": train_loss, "val_loss": val_loss}, step=epoch)

            print(f"Epoch [{epoch + 1}/{epochs}] | pAUC: {current_pauc:.4f} | AUC: {metrics['auc']:.4f}")

            if current_pauc > best_pauc:
                best_pauc = current_pauc
                torch.save(model.state_dict(), model_path)
                print(f"  üî• Saved Best Model (pAUC: {best_pauc:.4f})")

        # --- [C√ÅCH 1: TEST V·ªöI NG∆Ø·ª†NG T·ªêI ∆ØU] ---
        print("\nüß™ Testing Best Model V3 with Optimal Threshold...")
        if os.path.exists(model_path):
            model.load_state_dict(torch.load(model_path))

            # 1. T√¨m Best Threshold tr√™n t·∫≠p VALIDATION
            print("üîé Finding Best Threshold on Validation Set...")
            _, val_labels, val_probs = validate(model, val_loader, criterion)

            best_thresh = 0.5
            best_f1 = 0.0
            # Qu√©t t·ª´ 0.01 ƒë·∫øn 0.90
            for thr in np.arange(0.01, 0.91, 0.01):
                preds = (val_probs >= thr).astype(int)
                # T·ªëi ∆∞u h√≥a F1 cho l·ªõp Malignant (Label 1)
                score = f1_score(val_labels, preds, labels=[1], average='binary', zero_division=0)
                if score > best_f1:
                    best_f1 = score
                    best_thresh = thr

            print(f"‚úÖ Best Threshold Found: {best_thresh:.3f} (Val F1: {best_f1:.4f})")

            # 2. √Åp d·ª•ng Threshold ƒë√≥ l√™n t·∫≠p TEST
            test_loss, test_labels, test_probs = validate(model, test_loader, criterion)

            # T√≠nh metrics v·ªõi threshold t·ªëi ∆∞u
            test_metrics = calculate_metrics(test_labels, test_probs, threshold=best_thresh)

            print(f"üèÜ FINAL TEST V3 (Threshold {best_thresh:.3f})")
            print(f"pAUC (0.01): {test_metrics['pauc_0.01']:.4f}")
            print(f"AUC Full   : {test_metrics['auc']:.4f}")
            print(classification_report(test_labels, (test_probs >= best_thresh).astype(int),
                                        target_names=['Benign', 'Malignant']))

            # Log k·∫øt qu·∫£ cu·ªëi c√πng
            mlflow.log_metrics({f"test_{k}": v for k, v in test_metrics.items()})
            mlflow.log_param("best_threshold", best_thresh)

        else:
            print("‚ö†Ô∏è Warning: Model checkpoint not found.")


if __name__ == '__main__':
    train()